# Clustering 

This week we are learning how to use various clustering algorithms from sklearn. 
This is useful for making sense of ***unlabelled data***: observations for which we have certain metrics, based on which we try to predict a class label (such as whether an email is spam or not-spam, or whether a given flower is of one type or another). 
We do not know how many classes or labels we should have and we do not have a set of annotated data.

<img src='https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning.png' width="300">

## Lab Structure

Please have a look at the [Walkthrough](Walkthrough/W5_Walkthrough_Clustering.ipynb) for an introduction to **two** clustering algorithms: *KMeans* and *hierarchical clustering*.
We'll also see how to plot dendrograms, which show the hierachical relationship between observations.
You will then get as a chance to practice with the help of the [exercise for this week](Exercises/Clustering_Exercise.ipynb).

## To Do at Home

Try to solve on your own the [exercise for this week](Exercises/Clustering_Exercise.ipynb).

**Note**: Clustering can be computation-intensive, which is why some cells in the exercise notebook take a bit of time to run. 
This is especially true for questions 2 (~2 mins) and 7 (~10 mins) of the exercise. 
While these cells run, you can simply assume that your code is correct (if no error code shows up in the first seconds) and look at the next questions. 
This means that you can fill the dots ( ... ) without running the cells while waiting for the other, computation-intensive, cell to display its full output.
